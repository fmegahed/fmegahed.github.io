---
title: | 
    Generative AI Applications and Opportunities
author: 'Fadel M. Megahed, PhD <br>[`r icons::icon_style(icons::fontawesome("google"), fill = "white")` Scholar](https://scholar.google.com/citations?user=6CTlKGMAAAAJ&hl=en&oi=ao) &nbsp; |  &nbsp;
[`r icons::icon_style(icons::fontawesome("twitter"), fill = "white")` @FadelMegahed](https://twitter.com/FadelMegahed) &nbsp; | &nbsp;
[`r icons::icon_style(icons::fontawesome("github"), fill = "white")` @fmegahed](https://github.com/fmegahed) &nbsp; | &nbsp;
[`r icons::icon_style(icons::fontawesome("paper-plane", style = "solid"), fill = "white")` fmegahed@miamioh.edu](mailto:fmegahed@miamioh.edu)</br> <br>
<u><b><font color="white">Joint work with:</b></u><br>
Ying-Ju (Tessa) Chen, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` University of Dayton](https://udayton.edu/directory/artssciences/mathematics/chen-ying-ju.php)<br/>
Allison Jones-Farmer, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` Miami University](https://miamioh.edu/fsb/directory/?up=/directory/farmerl2)<br>
Sven Knoth, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` Helmut-Schmidt-Universit√§t](https://www.hsu-hh.de/compstat/en/sven-knoth-2)<br/> 
Douglas C. Montgomery, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` Arizona State University](https://search.asu.edu/profile/10123)<br>
Hongyue Sun, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` University of Georgia](https://engineering.uga.edu/team_member/hongyue-sun/)<br>
Inez Zwetsloot, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` University of Amsterdam](https://www.uva.nl/en/profile/z/w/i.m.zwetsloot/i.m.zwetsloot.html)<br/><br/>'
date: 'May 20, 2025 | Webinar | ENBIS'
output:
  xaringan::moon_reader:
    self_contained: false
    css: [default, "style_files/fonts.css", "style_files/my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLanguage: ["r", "python"]
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: true
header-includes:  
  - "style_files/header.html"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      progress = FALSE, 
                      verbose = FALSE,
                      dev = 'png',
                      fig.height = 3,
                      dpi = 300,
                      fig.align = 'center')

options(htmltools.dir.version = FALSE)


miamired = '#C3142D'

if(require(pacman)==FALSE) install.packages("pacman")
if(require(devtools)==FALSE) install.packages("devtools")
if(require(countdown)==FALSE) devtools::install_github("gadenbuie/countdown")
if(require(xaringanExtra)==FALSE) devtools::install_github("gadenbuie/xaringanExtra")
if(require(emo)==FALSE) devtools::install_github("hadley/emo")
if(require(icons)==FALSE) devtools::install_github("mitchelloharawild/icons")

pacman::p_load(tidyverse, magrittr, lubridate, janitor, # data analysis pkgs
               DataExplorer, scales, plotly, calendR, pdftools, # plots
               tmap, sf, urbnmapr, tigris, # maps
               bibliometrix, # for bibliometric analysis of my papers
               gifski, av, gganimate, ggtext, glue, extrafont, # for animations
               emojifont, emo, RefManageR, xaringanExtra, countdown) # for
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
if(require(xaringanthemer) == FALSE) install.packages("xaringanthemer")
library(xaringanthemer)

style_mono_accent(base_color = "#84d6d3",
                  base_font_size = "20px")

xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         
  mute_unhighlighted_code = TRUE  
)

xaringanExtra::use_xaringan_extra(c("tile_view", "animate_css", "tachyons", "panelset", "share_again", "search", "fit_screen", "editable", "clipboard"))

```


## Audience Pulse: Interests & Experience with Gen AI

<center>
<iframe src="https://wall.sli.do/event/5jdmw1sTzECQCYKjq4PE2U?section=05462fb2-179e-4eb3-a025-72619b3dddde" height="75%" width="75%" frameBorder="0" style="min-height: 450px;" allow="clipboard-write" title="Slido"></iframe></center>


.footnote[
<html>
<hr>
</html>

**Note:** Please scan the QR code to access the Slido poll. You can also use the link: <https://app.sli.do/event/5jdmw1sTzECQCYKjq4PE2U>. ![]()
]

---

## Just Confirming this Talk is not About Recycling!!

```{r recycling, echo=FALSE, out.width='45%'}

knitr::include_graphics('figs/oops_email.png')

```

---
count: false

## Just Confirming this Talk is not About Recycling!!

```{r recycling2, echo=FALSE, out.width='73%'}
knitr::include_graphics('figs/recycling2.png')
```

.footnote[
<html>
<hr>
</html>

**Note:** Cartoon created using ChatGPT-o3 on May 18, 2025. You can see the entire interaction [here](https://chatgpt.com/share/682aa227-8360-8005-b881-6d01414f1553). The full resolution image from that interaction can be downloaded [here](https://chatgpt.com/share/682aa14e-38f4-8191-a28f-1c7786e4bbe7).
]

---

## The Road to Large Language Models

<br>

```{r generative_ai_chart_out, out.width='100%', dpi = 600, fig.alt='From big data to big models, a flow chart documenting how we got to large language models'}
knitr::include_graphics('figs/generative_ai_chart.png')
```

.footnote[
<html>
<hr>
</html>

**Comment:** You have been hearing about **big data** in Quality Control for over a decade now. In fact, we presented our paper, [Statistical Perspectives on Big Data](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ab40f392e653b7336cbebf7c4fb95d3988748282), almost exactly 12 years ago in the ISQC Workshop in Sydney. We now have models that can digest questions/prompts and generate answers based on more than 45TB of text. 
]

---

## Uniqueness of LLMs vs. Earlier AI Models

.content-box-gray[
.bold[.red[LLMs:]] .bold[The objective is to generate new content rather than analyze existing data.]
]

.font90[
-  The generated content is based on a .bold[.red[stochastic behavior embedded in generative AI models such that the same input prompts results in different content]].
- LLMs with known model sizes can have up to **540 billion parameters** ([PaLM](https://arxiv.org/abs/2204.02311)). Note that state-of-the-art models like *GPT-o3*, and *Claude Sonnet 3.7*  **have not revealed their model sizes**.
- With the increase in model size, researchers have observed the **‚Äúemergent abilities‚Äù** of LLMs, which were **not explicitly encoded in the training**. [Examples include](https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html):
  + Multi-step arithmetic, and  
  + taking college-level exams.  
- LLMs are **foundation models** (see [Bommasani et al. 2021](https://arxiv.org/abs/2108.07258)), large pre-trained AI systems
that can be **repurposed with minimal effort across numerous domains and diverse tasks.**
]


---

## Generative AI Hype (2023)

```{r mckinsey_ai, echo=FALSE, out.width='60%'}
knitr::include_graphics('figs/mckinsey_ai.png')
```

.footnote[
<html>
<hr>
</html>

**Image Source:** [McKinsey & Company (July 2023). The economic potential of generative AI: The next productivity frontier [P. 10]](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#/)
]


---

## Generative AI Hype (2024)

.pull-left[
```{r google_ai, echo=FALSE, out.width='100%'}
knitr::include_graphics('figs/google_ai.png')
```

.center[
.font80[Andrew McAfee (2024). [Generally Faster: The Economic Impact of Generative AI](https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/Generally_Faster_-_The_Economic_Impact_of_Generative_AI.pdf)]
]
]

.pull-right[
```{r ssn_2024, echo=FALSE, out.width='100%'}
knitr::include_graphics('figs/gen_ai_coders_sept_2024.jpeg')
```

.center[.font80[Cui et al. (2024). [SSRN 4945566](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4945566)]]

]


---

## Generative AI Hype (2025)

```{r bcg_data, echo=FALSE, out.width='100%', fig.height = 3.5}
# Prepare data
df <- tibble::tibble(
  Country = factor(c("Japan", "US", "Singapore", "UK", "France", "Germany",
                     "India", "UAE", "Spain", "Italy", "Brazil", "Global"),
                   levels = rev(c("Japan", "US", "Singapore", "UK", "France", "Germany",
                                  "India", "UAE", "Spain", "Italy", "Brazil", "Global"))),
  `Up to $25M` = c(53, 59, 63, 65, 69, 69, 71, 78, 81, 83, 86, 69),
  `$26M-$50M`  = c(26, 23, 25, 18, 17, 18, 15, 15, 6, 12, 12, 18),
  `$51M-$100M` = c(10, 9, 6, 10, 6, 9, 8, 6, 5, 3, 0, 7),
  `>$100M`     = c(11, 9, 6, 7, 8, 4, 6, 1, 8, 2, 2, 6)
)

# Transform to long format
df_long <- tidyr::pivot_longer(
  data = df,
  cols = -Country,
  names_to = "Investment",
  values_to = "Percent"
)

# Order levels
df_long$Investment <- factor(df_long$Investment, 
                             levels = rev(c("Up to $25M", "$26M-$50M", "$51M-$100M", ">$100M")))

# Cumulative position for centering
df_long <- dplyr::group_by(df_long, Country) |> 
  dplyr::mutate(pos = cumsum(Percent) - Percent / 2)

# Add dynamic text color and conditional labels
df_long <- dplyr::mutate(
  df_long,
  text_color = ifelse(Percent > 15, "white", "white"),
  label = ifelse(Percent >= 3, paste0(Percent, "%"), "")
)

# Define fill colors from RColorBrewer
fill_colors <- c("black", miamired,  "#FEB24C", "#A9A9A9")
names(fill_colors) <- levels(df_long$Investment)

# Plot
ggplot2::ggplot(df_long, ggplot2::aes(x = Percent, y = Country, fill = Investment)) +
  ggplot2::geom_col(width = 0.8) +
  ggplot2::geom_text(
    ggplot2::aes(label = label, x = pos, color = text_color),
    size = 3,
    fontface = "bold",
    show.legend = FALSE
  ) +
  ggplot2::scale_x_continuous(
    breaks = seq(0, 100, 20),
    labels = scales::percent_format(scale = 1),
    limits = c(0, 120),
  ) +
  ggplot2::scale_color_identity() +
  ggplot2::scale_fill_manual(
    values = fill_colors,
    guide = ggplot2::guide_legend(reverse = TRUE)
    ) +
  ggplot2::labs(
    title = "How much are you planning to invest in AI in 2025?",
    x = "Percentage of companies", 
    y = NULL, 
    fill = NULL
    # caption = "**Data Source:** BCG AI Radar (2025). From Potential to Profit: Closing the AI Impact Gap. [P. 5] | **Adapted By:** Fadel M. Megahed"
  ) +
  ggplot2::theme_minimal(base_size = 10) +
  ggplot2::theme(
    legend.position = "bottom",
    panel.grid.major.y = ggplot2::element_blank(),
    panel.grid.minor = ggplot2::element_blank(),
    axis.text.y = ggplot2::element_text(face = "bold")
  ) +
  # Rectangle from 66 to 100% on the Global bar
  ggplot2::annotate(
    geom = "rect",
    xmin = 69, xmax = 100,
    ymin = 0.5, ymax = 1.5,
    fill = NA, color = "forestgreen", size = 1.5
  ) +
  
  # Label to the right of the bar
  ggplot2::annotate(
    geom = "label",
    x = 102, y = 1.35,
    label = "~ One in three \ncompanies plan \nto invest >$25M",
    hjust = 0, vjust = 0.5,
    size = 2,
    label.size = 0.3,
    fill = "white", color = "forestgreen",
    fontface = "bold"
  ) +
  ggplot2::theme(
    legend.position = "bottom",
    panel.grid.major.y = ggplot2::element_blank(),
    panel.grid.minor = ggplot2::element_blank(),
    axis.text.y = ggplot2::element_text(face = "bold"),
    axis.text.x = ggplot2::element_text(face = "bold"),
    legend.text = ggplot2::element_text(face = "bold"),
    plot.title = ggplot2::element_text(face = "bold", size = 12, hjust = 0.5),
    axis.title.x = ggplot2::element_text(face = "bold", size = 11),
    plot.caption = ggtext::element_markdown(size = 7, hjust = 0)
  ) 


```

.footnote[
<html>
<hr>
</html>

**Image Adapted from:** BCG AI Radar (2025). [From Potential to Profit: Closing the AI Impact Gap](https://www.bcg.com/publications/2025/how-much-are-you-planning-to-invest-in-ai-in-2025). [P. 5]
]


---

##ü§¶üèΩ‚Äç‚ôÇÔ∏èÔ∏è But also My Experience in May 2025 (ChatGPT-o3)

```{r rock_paper, echo=FALSE, out.width='75%'}
knitr::include_graphics('figs/rock_paper_scissors.png')
```

.footnote[
<html>
<hr>
</html>

**Source:** Megahed, F.M. (2025). [Playing Rock Paper Scissors with ChatGPT o3 on May 18, 2025.](https://chatgpt.com/share/6829d3fa-2640-8005-a882-971d1190bb21). See the full video of the interaction [here](https://www.loom.com/share/7c0eac514773425bb3b5ef0355add538?sid=cb1f9e28-bb71-4131-9cce-38d5dd79463c).
]


---

##ü§¶üèΩ‚Äç‚ôÇÔ∏èÔ∏è But also My Experience in May 2025 (ChatGPT-o3)


<center>
<iframe width="800" height="450" src="https://www.loom.com/embed/44c2ddbd1696471f93601573155b4f34?sid=60a80e88-97c3-4f67-b784-00b68e77caa6" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</center>

.footnote[
<html>
<hr>
</html>

**Context:** Here, we show that *ChatGPT-o3* (OpenAI's most advanced reasoning models) continues to hallucinate when asked about **fictional data not in the `qcc` package**. This demonstrates that the issues identified earlier by [Megahed et al. (2024)](https://www.tandfonline.com/doi/abs/10.1080/08982112.2023.2206479) remain relevant, despite the model's advancements.
]


---
class: inverse, center, middle

# Structured Text Extraction for SQC Applications


---

## What do we mean by "Structured Text Extraction"?

**Structured Text Extraction** is the process of extracting relevant information from unstructured text data and converting it into a structured format that can be easily analyzed.

<br>

```{r structured_text, echo=FALSE, out.width='100%'}
set.seed(2025)
recalls = readr::read_delim(
  "data/FLAT_RCL.txt", 
  delim = "\t",
  # based on https://static.nhtsa.gov/odi/ffdd/rcl/RCL.txt (name or desc based)
  col_names = c(
    "record_id", "campaign_number", "make", "model", "year", 
    "manf_campaign_number", "component_desc", "manufacturer", "begin_manf_date",
    "end_manf_date", "vehicle_equipment_tire_report", "potential_vehicles_affected",
    "date_owner_notified_by_manf", "recall_initatior", "manufactuerers_of_recalled",
    "report_receieved_date", "record_creation_date", "regulation_part_number",
    "federal_motor_vehicle_safety_standard_number", "defect_description",
    "consequence_description", "corrective_action_description", "recall_notes",
    "recall_component_id", "manufacturer_component_name", 
    "manufacturer_component_description",
    "manufacturer_component_number"
  )
) |> 
  # removing unknown years
  dplyr::filter(year >= 2025 & year < 9999) |> 
  # keeping only unique recall_descriptions since a recall can impact multiple models and makes
  dplyr::distinct(defect_description, .keep_all = TRUE) |> 
  dplyr::select(
    record_id, defect_description
  ) |> dplyr::sample_n(3)

knitr::kable(
  recalls,
  col.names = c("Record ID", "Defect Description"),
  caption = "2025 Vehicle Recall Data from NHTSA.",
  format = "html",
  escape = FALSE
) |> 
  kableExtra::kable_styling(
    full_width = FALSE,
    position = "left",
    html_font = "Arial",
    font_size = 12,
    bootstrap_options = c("striped", "hover", "condensed")
  )

```


---

## The Code and Output

.font90[
```{r structured_text_code, echo = TRUE, results='hide', out.width='100%'}
chat = ellmer::chat_ollama(model = 'gemma3:27b')

type_recall = ellmer::type_object(
  manufacturer   = ellmer::type_string("The name of the company recalling the vehicles.", required=F),
  models         = ellmer::type_string("List of affected vehicle models.", required=F),
  model_years    = ellmer::type_string("List of model years affected.", required=F),
  defect_summary = ellmer::type_string("Summary of the main defect.", required=F)
)

extract_fn = function(x, chat_object, custom_type_object){
  return( chat_object$extract_data(x, type = custom_type_object) )
}

extracted_data = purrr::map_df(
  .x=recalls$defect_description, .f=extract_fn, chat_object=chat, custom_type_object=type_recall
  ) 
```
]

```{r structured_text_code2, echo=FALSE, out.width='100%'}
knitr::kable(
  extracted_data |> 
    dplyr::mutate(record_id = recalls$record_id) |> 
    dplyr::select(
      record_id, manufacturer, models, model_years, defect_summary
      ) |> 
    tail(2),
  col.names = c("Record ID", "Manufacturer", "Vehicle Models", "Model Years", "Defect Summary"),
  format = "html",
  escape = FALSE
) |> 
  kableExtra::kable_styling(
    full_width = FALSE,
    position = "left",
    html_font = "Arial",
    font_size = 12,
    bootstrap_options = c("striped", "hover", "condensed")
  )
```

.footnote[
<html>
<hr>
</html>

**Note:** The choice of open-source vs closed-source model, and the model size can have a large impact on the performance of this step. **Here**, we only show the last two rows of our extraction step for space considerations.
]


---

## Our No-Code App for STE: A Demo

Our [no-code app](https://huggingface.co/spaces/fmegahed/structured_text_extraction) utilizes the `ellmer` package to extract structured data from unstructured text, and OpenAI's `gpt-4o-mini` model to perform the extraction. 

```{r structured_text_app, echo=FALSE, out.width='90%'}
knitr::include_graphics('figs/hf_ste.png')
```

---

## Some Potential Applications of STE in SQC

- **Defect Detection**: Automating the identification of defects in manufacturing processes by analyzing inspection reports and maintenance logs.  

- **Customer Feedback Analysis**: Analyzing customer feedback, reviews, and warranty claims to identify common defects or issues with products. 

- **Regulatory Compliance**: Extracting information from regulatory documents, such as safety recalls or compliance reports, to ensure that products meet industry standards and regulations.  

- **Research:** Analyzing research papers, patents, and technical reports to extract relevant information about new materials, processes, or technologies.


---

## Practical Considerations

> LLMs are **inherently stochastic**. It is important to check the: (a) **consistency** of the extracted data, and (b) **external validity** of the results.  

```{r ste_reliability, echo=FALSE, out.width='45%'}
knitr::include_graphics('figs/intra_dist.png')
```


.footnote[
<html>
<hr>
</html>

**Note:** Our work on LLM consistency should be on **arXiv** sometime next week. Our detailed analysis in this area is available [here](https://fmegahed.github.io/research/llm_consistency/llm_consistency.html).
]


---
class: inverse, center, middle

# ChatISA: Our In-House Bot for Students

<br>

.pull-left-2[<br>Megahed, F. M., Chen, Y. J., Ferris, J.A., Resatar, C., Ross, K., Lee, Y., & Jones-Farmer, L. A. (2024). ChatISA: A Prompt-Engineered Chatbot for Coding, Project Management, Interview and Exam Preparation Activities. Under review.

[Freely available @ [arXiv](https://arxiv.org/abs/2407.15010)].]

.pull-right-2[<div><img src="figs/paper3_qr_code.png" class="paper-img" width="300px" align="right"></div>]


---

## A Live Demo of ChatISA

<center>
    <a href="https://chatisa.fsb.miamioh.edu/">
        <img alt="The interface to our ChatISA app" src="figs/chatisa_demo.gif" style="width:80%; height:80%;">
    </a>
</center>

.footnote[
<html>
<hr>
</html>

**Note:** We encourage the audience to experiment with **ChatISA** at <https://chatisa.fsb.miamioh.edu/>. If we have time, we can also go over [this pre-recorded and sped-up demo of our voice-enabled Interview Mentor](https://www.loom.com/share/896d4ab0e18747f0bca8dba5fff9cc36?sid=24872af4-aefd-4889-8083-b3eeb1082c52).
]



---
class: inverse, center, middle

# ChatSQC: Our Grounded App, to address Imprecise SQC Answers and Hallucinations

<br>

.pull-left-2[<br>Megahed, F. M., Chen, Y. J., Zwetsloot, I., Knoth, S., Montgomery, D.C., & Jones-Farmer, L. A. (2024). Introducing ChatSQC: Enhancing Statistical Quality Control with Augmented AI.  *Journal of Quality Technology*, 56(5), 474-497.

[Freely available @ [arXiv](https://arxiv.org/pdf/2308.13550)].]

.pull-right-2[<div><img src="figs/paper2_qr_code.png" class="paper-img" width="300px" align="right"></div>]



---

## The Construction of ChatSQC

```{r chatsqc, out.width='80%', echo=FALSE, fig.alt='The construction of ChatSQC involved four main phases: (a) a one-time extraction of the reference material, (b) a one-time preprocessing of the extracted material, (c) a continuous (online) chat inference, and (d) the hosting/deployment of the app on a web server.'}
knitr::include_graphics('figs/ChatSQC_flowchart_new.png')
```

---

## A Live Demo of ChatSQC

<center>
    <a href="https://chatsqc.osc.edu/">
        <img alt="The interface to our ChatSQC app" src="figs/chatsqc_demo.png" style="width:90%; height:90%;">
    </a>
</center>

.footnote[
<html>
<hr>
</html>

**Note:** We encourage the audience to experiment with **ChatSQC** at <https://chatsqc.osc.edu/>.
]


---
class: inverse, center, middle

# Adapting OpenAI's CLIP Model for Few-Shot Image Inspection in Manufacturing Quality Control

<br>

.pull-left-2[<br>Megahed, F. M., Chen, Y. J., Colosimo, B. M., Grasso, M. L. G., Jones-Farmer, L. A., Knoth, S., Sun, H. & Zwetsloot, I. (2025). Adapting OpenAI's CLIP Model for Few-Shot Image Inspection in Manufacturing Quality Control: An Expository Case Study with Multiple Application Examples. arXiv preprint arXiv:2501.12596.

[Freely available @ [arXiv](https://arxiv.org/pdf/2501.12596)].]

.pull-right-2[<div><img src="figs/paper4_qr_code.png" class="paper-img" width="300px" align="right"></div>]


---

## What is CLIP?


```{r clip, echo=FALSE, out.width='100%'}
knitr::include_graphics('figs/CLIP.png')
```


.footnote[
<html>
<hr>
</html>

**Note:** CLIP (Contrastive Language‚ÄìImage Pretraining) is a neural network architecture that learns to connect images and text. It was developed by OpenAI and released in January 2021.

**Image Source:** <https://github.com/openai/CLIP>.
]


???

This is the main training loss for CLIP. The key idea is simple: we want the embedding of an image and its correct caption to be close, and for all other captions to be far away ‚Äî and vice versa.

So for each image‚Äìtext pair in the mini-batch, we compute how similar the image is to every text in the batch, and how similar the text is to every image. Then we use a softmax to convert those similarities into a probability distribution, and apply cross-entropy loss to maximize the likelihood of the correct match.

The loss is symmetric ‚Äî we do this from image-to-text and text-to-image, and average the two directions.

This simple structure is what gives CLIP its ability to generalize so broadly.

---

## Our Few Shot QC Framework

```{r few_shot, echo=FALSE, out.width='100%'}
knitr::include_graphics('figs/clip_framework.png')
```

> **Practical Notes:** 
>
> All preprocessing shown in the figure‚Äîcenter-crop ‚Üí resize ‚Üí patch extraction ‚Üí embedding‚Äîhappens **automatically** inside the standard `model, preprocess = clip.load("ViT-L/14", device=device)` function.
>
> In practice, you call one function, and CLIP handles the crop, resize, non-overlapping patching, and projection to the $d$-dimensional vector.*


---

## An Example with Stochastic Textured Surfaces (STS)

```{r exp03_learning_images, echo=FALSE, out.width='100%'}
knitr::include_graphics('https://raw.githubusercontent.com/fmegahed/qe_genai/main/results/exp03_learning_images.gif')
```



---

## Zero-Shot Failure for the STS Example


CLIP fails in **zero‚Äëshot** because STS defects are subtle & domain‚Äëspecific.

<br>

| Accuracy | Sens | Spec | Prec | F1 | AUC |
|---------:|-----:|-----:|-----:|---:|----:|
| 0.500 | 0.000 | 1.000 | 0.000 | 0.000 | 0.171 |

<br>

Therefore we evaluate **few‚Äëshot** performance while varying the learning‚Äëset size (*ViT‚ÄëL/14* vs *ViT‚ÄëB/32*).


---

## Few-Shot Learning for the STS Example

```{r exp03_few_shot, echo=FALSE, out.width='72%'}
knitr::include_graphics('figs/exp03_comparing_two_clip_models.png')
```


???

- *ViT‚ÄëL/14* reaches **97% accuracy** with only **50** images/class.  

- Smaller *ViT‚ÄëB/32* saturates around 76%.  

- Fine‚Äëgrained $14\times14$ patching better captures local stochastic texture.



---

## Our No-Code App for CLIP: A Demo

**Our tool is available at**: <https://huggingface.co/spaces/fmegahed/clip>.

```{r clip_demo, echo=FALSE, out.width='100%'}
knitr::include_graphics('figs/hf_clip.png')
```



---

class: inverse, center, middle

# Concluding Remarks


---

