<!DOCTYPE html>
<html lang="en"><head>
<script src="clip_files/libs/clipboard/clipboard.min.js"></script>
<script src="clip_files/libs/quarto-html/tabby.min.js"></script>
<script src="clip_files/libs/quarto-html/popper.min.js"></script>
<script src="clip_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="clip_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="clip_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="clip_files/libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="Fadel M. Megahed">
  <meta name="author" content="Ying-Ju Chen">
  <meta name="author" content="Bianca Colosimo">
  <meta name="author" content="Marco Grasso">
  <meta name="author" content="Allison Jones-Farmer">
  <meta name="author" content="Sven Knoth">
  <meta name="author" content="Hongyue Sun">
  <meta name="author" content="Inez Zwetsloot">
  <meta name="dcterms.date" content="2025-08-05">
  <title>Adapting OpenAI’s CLIP Model for Few-Shot Image Inspection in Manufacturing Quality Control</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="clip_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="clip_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="clip_files/libs/revealjs/dist/theme/quarto-6119baf752452144a39fe49f5cddfd0e.css">
  <link href="clip_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="clip_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="clip_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="clip_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="clip_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="clip_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-color="#c3142d" data-background-image="pres_qr_code.png" data-background-position="99% 95%" data-background-size="10%" class="quarto-title-block center">
  <h1 class="title">Adapting OpenAI’s CLIP Model for Few-Shot Image Inspection in Manufacturing Quality Control</h1>
  <p class="subtitle">An Expository Case Study with Multiple Application Examples</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Fadel M. Megahed 
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Ying-Ju Chen 
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Bianca Colosimo 
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Marco Grasso 
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Allison Jones-Farmer 
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Sven Knoth 
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Hongyue Sun 
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Inez Zwetsloot 
</div>
</div>
</div>

  <p class="date">August 05, 2025</p>
</section><section id="TOC">
<nav role="doc-toc"> 
<h2 id="toc-title">Table of contents</h2>
<ul>
<li><a href="#/context-and-motivation" id="/toc-context-and-motivation">Context and Motivation</a></li>
<li><a href="#/background-what-is-contrastive-language-image-pretraining-clip" id="/toc-background-what-is-contrastive-language-image-pretraining-clip">Background: What is Contrastive Language-Image Pretraining (CLIP)?</a></li>
<li><a href="#/our-few-shot-learning-adaptation-for-clip" id="/toc-our-few-shot-learning-adaptation-for-clip">Our Few-Shot Learning Adaptation for CLIP</a></li>
<li><a href="#/concluding-remarks" id="/toc-concluding-remarks">Concluding Remarks</a></li>
</ul>
</nav>
</section>
<section>
<section id="context-and-motivation" class="title-slide slide level1 center inverse">
<h1>Context and Motivation</h1>

</section>
<section id="the-evolution-of-statistical-process-monitoring-spm" class="slide level2">
<h2>The Evolution of Statistical Process Monitoring (SPM)</h2>
<p><br></p>

<img data-src="figs/spm_evolution.png" class="r-stretch"><div class="footnote">
<p><strong>Image Source:</strong> Colosimo BM, Jones-Farmer LA, Megahed FM, Paynabar K, Ranjan C, Woodall WH. Statistical process monitoring from industry 2.0 to industry 4.0: insights into research and practice. Technometrics. 2024 Oct 1;66(4):507-30. The paper is publicly available at the <a href="https://re.public.polimi.it/bitstream/11311/1268020/1/0Statistical%20Process%20Monitoring%20from%20Industry%202.0%20to%20Industry%204.0%20%20Insights%20into%20Research%20and%20Practice.pdf">first author’s repo (click here)</a> to access it – see Figure 1.</p>
</div>
</section>
<section id="key-technologies-enabling-spm-4.0" class="slide level2">
<h2>Key Technologies Enabling SPM 4.0</h2>

<img data-src="figs/spm_tech.png" class="quarto-figure quarto-figure-center r-stretch" style="width:20.0%"></section>
<section id="what-is-an-image" class="slide level2">
<h2>What is an Image?</h2>

<img data-src="figs/microstructure_animation.gif" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="typical-pipeline-for-image-based-spm" class="slide level2">
<h2>Typical Pipeline for Image-Based SPM</h2>
<p><strong>Prior to image monitoring, several preprocessing steps are typically applied.</strong></p>

<img data-src="figs/fig5.png" class="quarto-figure quarto-figure-center r-stretch" style="width:100.0%"><ul>
<li><strong>Raw Image</strong>: Direct camera output, often affected by noise, lighting, or alignment variability.</li>
<li><strong>Normalization</strong>: Standardizes intensity ranges across images, minimizing lighting differences.</li>
<li><strong>Edge Detection</strong>: Enhances structural elements like cracks or seams, highlighting defects.</li>
<li><strong>Feature Extraction</strong>: Summarizes key spatial information (e.g., region intensities, patterns) into compact descriptors.</li>
<li><strong>Monitoring Statistic</strong>: Uses extracted features in control charts (e.g., GLR) to detect when and where changes occur.</li>
</ul>
</section>
<section id="industrial-challenges-for-practical-adoption-of-image-based-spm" class="slide level2">
<h2>Industrial Challenges for Practical Adoption of Image-Based SPM</h2>
<p>In our opinion, there are <strong>three main challenges that hinder the wide adoption of image-based SPM methods</strong>.</p>

<img data-src="figs/challenges.png" class="quarto-figure quarto-figure-center r-stretch" style="width:100.0%"><p><br></p>
<h3 id="but">But, …</h3>
<p>We have <strong>two built-in advantages:</strong></p>
<ul>
<li>SPM methods can <strong>accumulate evidence over time</strong> (e.g., GLR-type charts)<br>
</li>
<li>SPM methods do not require <strong>a representative training set of defective</strong> images</li>
</ul>
</section>
<section id="our-research-objetive-and-goals" class="slide level2">
<h2>Our Research Objetive and Goals</h2>
<blockquote>
<p><strong>Objective</strong>: We propose a <strong>few-shot learning CLIP-based approach</strong> as a simple yet powerful baseline and preliminary benchmark, addressing the aforementioned limitations.</p>
</blockquote>
<ol type="1">
<li><p><strong>Demonstrate the practical use of CLIP’s few-shot learning</strong><br>
Easily adapt CLIP for manufacturing quality control, and show that it can offer high accuracy with reduced complexity.</p></li>
<li><p><strong>Evaluate the effect of learning set size on performance</strong><br>
Analyze how training data volume influences classification accuracy and offer guidance on minimal data requirements for successful application.</p></li>
<li><p><strong>Compare different vision transformer backbones</strong><br>
Benchmark ViT-B/32 vs.&nbsp;ViT-L/14 in terms of classification accuracy and computational efficiency, helping practitioners make informed model choices.</p></li>
</ol>
</section></section>
<section>
<section id="background-what-is-contrastive-language-image-pretraining-clip" class="title-slide slide level1 center inverse">
<h1>Background: What is Contrastive Language-Image Pretraining (CLIP)?</h1>

</section>
<section id="what-is-clip" class="slide level2">
<h2>What is CLIP?</h2>
<ul>
<li><p><strong>CLIP</strong> stands for <strong>Contrastive Language-Image Pre-training</strong>.</p></li>
<li><p>It was developed by <a href="https://openai.com/index/clip/">OpenAI</a> and trained on a <strong>massive dataset of 400 million image-text pairs</strong>.</p></li>
<li><p>The core idea is to <strong>teach an AI to understand images and the words we use to describe them</strong>.</p></li>
<li><p>After training, <a href="https://openai.com/index/clip/">CLIP can be used to</a>:</p>
<ul>
<li><p>Classify images using only text descriptions (<strong>zero-shot classification</strong>).</p></li>
<li><p>Power applications like image search and automated captioning.</p></li>
</ul></li>
</ul>
</section>
<section id="a-visual-introduction-to-clip" class="slide level2">
<h2>A Visual Introduction to CLIP</h2>

<img data-src="figs/CLIP.png" class="r-stretch"><ul>
<li>CLIP uses <strong>two encoders</strong>: one for images, one for text<br>
</li>
<li>The model learns to match the correct image with the correct text</li>
</ul>
<div class="footnote">
<p><strong>Image Source:</strong> <a href="https://github.com/openai/CLIP">OpenAI’s GitHub Repo for CLIP</a></p>
</div>
</section>
<section id="an-illustrative-example-the-ask" class="slide level2">
<h2>An Illustrative Example: The Ask?</h2>
<div class="columns">
<div class="column">
<p>Let’s test CLIP with an image showing a metal component with a defect.</p>
<p>Our question is <strong>simple</strong>:</p>
<blockquote>
<p>Can CLIP correctly identify this as a defective part?</p>
</blockquote>
<p>We gave it <strong>five captions to choose from:</strong></p>
<ol type="1">
<li><p>“A defective metal component”</p></li>
<li><p>“A nominal metal component”</p></li>
<li><p>“An industrial part”</p></li>
<li><p>“A piece of sheet metal”</p></li>
<li><p>“An artistic photograph”</p></li>
</ol>
</div><div class="column">
<p><br></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/IMG_1572.jpeg"></p>
<figcaption>A metal pan with a simulated defect (black Sharpie line)</figcaption>
</figure>
</div>
</div></div>
</section>
<section id="an-illustrative-example-clips-internal-preprocessing-and-encoding" class="slide level2">
<h2>An Illustrative Example: CLIP’s Internal Preprocessing and Encoding</h2>
<ol type="1">
<li><strong>Image Preprocessing</strong> (based on the <em>ViT-L/14</em> CLIP model (encoder))
<ul>
<li>Original resolution: 3264×2448 pixels<br>
</li>
<li>CLIP center-crops and resizes it to <strong>336×336 pixels</strong></li>
</ul></li>
<li><strong>Image Embedding</strong>
<ul>
<li>The preprocessed image is encoded into a <strong>768-dimensional vector</strong>:<br>
<span class="math inline">\(\mathbf{v} = [0.371,\ 0.857,\ -0.063, \ \ldots,\ -0.649,\ -0.153,\ -0.198]\)</span></li>
</ul></li>
<li><strong>Provided Text Description and its Embedding (Example: a)</strong>
<ul>
<li>“A defective metal component” is padded and tokenized into 77 tokens<br>
</li>
<li>Then encoded to:<br>
<span class="math inline">\(\mathbf{t} = [-0.0135,\ -0.0178,\ -0.0316,\ \ldots,\ 0.0006,\ 0.0008,\ 0.0638]\)</span></li>
</ul></li>
<li><strong>Similarity Comparison</strong>
<ul>
<li>CLIP computes <strong>cosine similarity</strong> between <span class="math inline">\(\mathbf{v}\)</span> and each <span class="math inline">\(\mathbf{t}_i\)</span></li>
</ul></li>
</ol>
</section>
<section id="an-illustrative-example-our-obtained-zero-shot-classfication-results" class="slide level2">
<h2>An Illustrative Example: Our Obtained Zero-Shot Classfication Results</h2>
<p>CLIP computes the <strong>cosine similarity</strong> between them, which we then passed through the <strong>softmax function</strong> to produce the following obtained probabilities:</p>
<p><br></p>
<table style="width: 70%; table-layout: fixed; border-collapse: collapse; font-size: 0.95em;">
<colgroup>
<col style="width: 17%;">
<col style="width: 45%;">
</colgroup>
<thead>
<tr style="background-color: #c3142d; color: white;">
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
Probability
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: left;">
Text Description
</th>
</tr>
</thead>
<tbody>
<tr style="background-color: white;">
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
<strong>85.1%</strong>
</td>
<td style="padding: 8px; border: 1px solid #ddd;">
A <strong>nominal</strong> metal component
</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
6.2%
</td>
<td style="padding: 8px; border: 1px solid #ddd;">
A defective metal component
</td>
</tr>
<tr style="background-color: white;">
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
4.9%
</td>
<td style="padding: 8px; border: 1px solid #ddd;">
An industrial part
</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
3.8%
</td>
<td style="padding: 8px; border: 1px solid #ddd;">
A piece of sheet metal
</td>
</tr>
<tr style="background-color: white;">
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
0.01%
</td>
<td style="padding: 8px; border: 1px solid #ddd;">
An artistic photograph
</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Despite the simulated defect, CLIP predicts <strong>“nominal”</strong> as the most likely label.</p>
</section>
<section id="takeaway-from-our-illustrative-example" class="slide level2">
<h2>Takeaway from our Illustrative Example</h2>
<p>While the <strong>purpose</strong> of this illustrative example was to demonstrate how zero-shot classification works with the CLIP model, it seems to be consistent with the published limitations of the CLIP model.</p>
<blockquote>
<p>“CLIP also still has <strong>poor generalization to images not covered in its pre-training dataset</strong>.” <a href="https://openai.com/index/clip/">OpenAI Blog Post on the CLIP Model</a></p>
</blockquote>
<p><br></p>
<blockquote>
<p>We suspect that the majority of industrial SPM applications to fit into the <strong>images not covered in its pretraining dataset</strong> category, which can overcome using few-shot learning examples.</p>
</blockquote>
</section></section>
<section>
<section id="our-few-shot-learning-adaptation-for-clip" class="title-slide slide level1 inverse center">
<h1>Our Few-Shot Learning Adaptation for CLIP</h1>

</section>
<section id="our-few-shot-framework-for-clip-based-quality-inspection" class="slide level2">
<h2>Our Few-Shot Framework for CLIP-Based Quality Inspection</h2>

<img data-src="figs/clip_framework.png" class="r-stretch"><blockquote>
<p><strong>Practical Notes:</strong></p>
<p>All preprocessing in the center block happens <strong>automatically</strong> inside the standard <code>model, preprocess = clip.load("ViT-L/14", device=device)</code> function.</p>
<p>In practice, you call one function, and CLIP handles the crop, resize, non-overlapping patching, and projection to the <span class="math inline">\(d\)</span>-dimensional vector.</p>
<p>We do <strong>NOT</strong> leverage the <strong>text encoder</strong> in our few-shot approach. We only use the <strong>image encoder</strong> to extract the <span class="math inline">\(d\)</span>-dimensional vector representation of the image.</p>
</blockquote>
</section>
<section id="the-pan-example-how-we-expanded-on-the-illustrative-example" class="slide level2">
<h2>The Pan Example: How we Expanded on the Illustrative Example?</h2>
<ul>
<li><strong>Binary Classification:</strong> Reduced from five classes to two – <em>nominal</em> vs.&nbsp;<em>defective</em>.
<ul>
<li><em>“A metallic pan free of black scuff marks”</em><br>
</li>
<li><em>“A metallic pan with a simulated scuff mark drawn by a black marker”</em></li>
</ul></li>
<li>Tested on <strong>78 Images:</strong> 39 nominal, 39 defective (instead of just one!!).</li>
</ul>
<div style="margin-top: -1.15em;">

</div>

<img data-src="https://raw.githubusercontent.com/fmegahed/qe_genai/main/results/exp01_learning_images.gif" class="quarto-figure quarto-figure-center r-stretch" width="800"></section>
<section id="zero-shot-results-for-the-pan-example" class="slide level2">
<h2>Zero-Shot Results for the Pan Example</h2>
<p>CLIP <strong>fails in zero‑shot because manufacturing defects are likely domain‑specific.</strong></p>
<p><br></p>
<table style="width: 90%; table-layout: fixed; border-collapse: collapse; font-size: 0.95em;">
<thead>
<tr style="background-color: #c3412d; color: white;">
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
Accuracy
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
Sensitivity (Recall)
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
Specificity
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
Precision
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
F1-Score
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
AUC
</th>
</tr>
</thead>
<tbody>
<tr style="background-color: white;">
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
0.615
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
0.231
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
1.000
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
1.000
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
0.375
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
0.559
</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Therefore we evaluate our <strong>few‑shot</strong> performance.</p>
</section>
<section id="few-shot-results-for-the-pan-example" class="slide level2">
<h2>Few-Shot Results for the Pan Example</h2>
<p><br></p>
<table style="width: 100%; table-layout: fixed; border-collapse: collapse; font-size: 0.75em;">
<thead>
<tr style="background-color: #c3412d; color: white;">
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
Model
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
Accuracy
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
Sensitivity
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
Specificity
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
Precision
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
F1
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
AUC
</th>
<th style="padding: 8px; border: 1px solid #ddd; text-align: center;">
Runtime (min)
</th>
</tr>
</thead>
<tbody>
<tr style="background-color: white;">
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
CLIP Few-Shot
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
<strong>0.910</strong>
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
<strong>0.821</strong>
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
<strong>1.000</strong>
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
<strong>1.000</strong>
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
<strong>0.901</strong>
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
<strong>0.998</strong>
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
9.1
</td>
</tr>
<tr style="background-color: #f9f9f9;">
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
<a href="https://link.springer.com/content/pdf/10.1007/s10845-010-0378-3.pdf" target="_blank" style="text-decoration: none; color: inherit;"> Benchmark
</a></td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
0.880
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
0.760
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
<strong>1.000</strong>
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
<strong>1.000</strong>
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
0.857
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
– –
</td>
<td style="padding: 8px; border: 1px solid #ddd; text-align: center;">
– –
</td>
</tr>
</tbody>
</table>
<p><br></p>
<ul>
<li><p><strong>CLIP Few-Shot performs at least as well as the <a href="https://link.springer.com/content/pdf/10.1007/s10845-010-0378-3.pdf">benchmark</a> across all reported metrics</strong>, with perfect specificity and precision, and a higher F1-score.</p></li>
<li><p>It also offers a <strong>simpler and more efficient implementation</strong>, requiring just 9.1 minutes of runtime (loading the 100 images, few-shot learning, and test classification).</p></li>
</ul>
</section>
<section id="the-stochastic-textured-surfaces-sts-example" class="slide level2">
<h2>The Stochastic Textured Surfaces (STS) Example</h2>

<img data-src="https://raw.githubusercontent.com/fmegahed/qe_genai/main/results/exp03_learning_images.gif" class="r-stretch"></section>
<section id="zero-shot-failure-for-the-sts-example" class="slide level2">
<h2>Zero-Shot Failure for the STS Example</h2>
<p>CLIP fails in <strong>zero‑shot</strong> because STS defects are subtle &amp; domain‑specific.</p>
<p><br></p>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: right;">Accuracy</th>
<th style="text-align: right;">Sens</th>
<th style="text-align: right;">Spec</th>
<th style="text-align: right;">Prec</th>
<th style="text-align: right;">F1</th>
<th style="text-align: right;">AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.500</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">0.171</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Therefore we evaluate <strong>few‑shot</strong> performance while varying the learning‑set siz and two image encoder models (<em>ViT‑L/14</em> vs <em>ViT‑B/32</em>).</p>
</section>
<section id="few-shot-learning-for-the-sts-example" class="slide level2">
<h2>Few-Shot Learning for the STS Example</h2>

<img data-src="figs/exp03_comparing_two_clip_models.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<ul>
<li><p><em>ViT‑L/14</em> reaches <strong>97% accuracy</strong> with only <strong>50</strong> images/class.</p></li>
<li><p>Smaller <em>ViT‑B/32</em> saturates around 76%.</p></li>
<li><p>Fine‑grained <span class="math inline">\(14\times14\)</span> patching better captures local stochastic texture.</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="concluding-remarks" class="title-slide slide level1 inverse center">
<h1>Concluding Remarks</h1>

</section>
<section id="key-takeaways" class="slide level2">
<h2>Key Takeaways</h2>
<ul>
<li><p>CLIP is a <strong>simple, yet effective baseline for image inspection before exploring more complex or costly alternatives</strong>.</p></li>
<li><p>Performance is <strong>strong in single-component, texture-rich applications, and weaker in multi-object, complex scenes</strong> (please refer to <a href="https://arxiv.org/abs/2501.12596">our paper</a>).</p></li>
<li><p><strong>Encoing Model choice matters:</strong> ViT-L/14 offers superior performance for fine-grained textures.</p></li>
<li><p>Effective with as few as <strong>10–50 images per class</strong> in many settings—ideal for data-scarce domains.</p></li>
</ul>
</section>
<section id="try-out-our-no-code-app-in-public-alpha" class="slide level2">
<h2>Try-Out Our No-Code App? In Public Alpha</h2>
<p>Our <strong>no-code tool is available at:</strong> <a href="https://huggingface.co/spaces/fmegahed/clip" class="uri">https://huggingface.co/spaces/fmegahed/clip</a>.</p>

<img data-src="figs/hf_clip.png" style="width:50.0%" class="r-stretch"><div class="footnote">
<p><strong>Source code:</strong> <a href="https://huggingface.co/spaces/fmegahed/clip/tree/main">HF Files</a> | <strong>Detailed experiments:</strong> <a href="https://github.com/fmegahed/qe_genai/blob/main/notebook/image_inspection_with_clip.ipynb">Python Notebook</a> | <strong>Python library:</strong> <a href="https://test.pypi.org/project/FewShotIQ/0.1.1/">FewShotIQ</a></p>
</div>
</section>
<section id="section" class="slide level2 inverse center title-slide">
<h2></h2>
<div style="position: absolute; bottom: -100px; right:-480px">
<p><img src="pres_qr_code.png" alt="Presentation QR Code" width="35%"></p>
</div>
<section id="title-slide" data-background-color="#c3142d" data-background-image="pres_qr_code.png" data-background-position="99% 95%" data-background-size="10%" class="quarto-title-block center">
<h1 class="title">
Adapting OpenAI’s CLIP Model for Few-Shot Image Inspection in Manufacturing Quality Control
</h1>
<p class="subtitle">
An Expository Case Study with Multiple Application Examples
</p>
<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<p>Fadel M. Megahed</p>
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<p>Ying-Ju Chen</p>
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<p>Bianca Colosimo</p>
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<p>Marco Grasso</p>
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<p>Allison Jones-Farmer</p>
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<p>Sven Knoth</p>
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<p>Hongyue Sun</p>
</div>
</div>
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<p>Inez Zwetsloot</p>
</div>
</div>
</div>

</section></section>
    </section></div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="https://miamioh.edu/miami-brand/_files/images/system/identity/logo-horizontal-stacked.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://ww3.aievolution.com/JSMAnnual2025/Events/viewEv?ev=1861">JSM 2025</a> | Nashville, TN | Presentation based on <a href="https://arxiv.org/abs/2501.12596">Megahed et al.&nbsp;(2025)</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="clip_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="clip_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="clip_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="clip_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="clip_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="clip_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="clip_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="clip_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="clip_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="clip_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="clip_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    

    <script>

      // htmlwidgets need to know to resize themselves when slides are shown/hidden.

      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current

      // slide changes (different for each slide format).

      (function () {

        // dispatch for htmlwidgets

        function fireSlideEnter() {

          const event = window.document.createEvent("Event");

          event.initEvent("slideenter", true, true);

          window.document.dispatchEvent(event);

        }

    

        function fireSlideChanged(previousSlide, currentSlide) {

          fireSlideEnter();

    

          // dispatch for shiny

          if (window.jQuery) {

            if (previousSlide) {

              window.jQuery(previousSlide).trigger("hidden");

            }

            if (currentSlide) {

              window.jQuery(currentSlide).trigger("shown");

            }

          }

        }

    

        // hookup for slidy

        if (window.w3c_slidy) {

          window.w3c_slidy.add_observer(function (slide_num) {

            // slide_num starts at position 1

            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);

          });

        }

    

      })();

    </script>

    

    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>