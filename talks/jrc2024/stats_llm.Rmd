---
title: | 
    Industrial Satistics and Large Language Models
author: 'Fadel M. Megahed, PhD <br>[`r icons::icon_style(icons::fontawesome("google"), fill = "white")` Scholar](https://scholar.google.com/citations?user=6CTlKGMAAAAJ&hl=en&oi=ao) &nbsp; |  &nbsp;
[`r icons::icon_style(icons::fontawesome("twitter"), fill = "white")` @FadelMegahed](https://twitter.com/FadelMegahed) &nbsp; | &nbsp;
[`r icons::icon_style(icons::fontawesome("github"), fill = "white")` @fmegahed](https://github.com/fmegahed) &nbsp; | &nbsp;
[`r icons::icon_style(icons::fontawesome("paper-plane", style = "solid"), fill = "white")` fmegahed@miamioh.edu](mailto:fmegahed@miamioh.edu)</br> <br>
<u><b><font color="white">Joint work with:</b></u><br>
Ying-Ju (Tessa) Chen, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` University of Dayton](https://udayton.edu/directory/artssciences/mathematics/chen-ying-ju.php)<br/>
Allison Jones-Farmer, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` Miami University](https://miamioh.edu/fsb/directory/?up=/directory/farmerl2)<br>
Sven Knoth, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` Helmut-Schmidt-Universit√§t](https://www.hsu-hh.de/compstat/en/sven-knoth-2)<br/> 
Younghwa (Gabe) Lee, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` Miami University](https://miamioh.edu/fsb/directory/?up=/directory/leeyh2)<br>
Douglas C. Montgomery, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` Arizona State University](https://search.asu.edu/profile/10123)<br>
Brooke Wang, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` Miami University](https://miamioh.edu/fsb/directory/?up=/directory/wangj249)<br>
Inez Zwetsloot, PhD &nbsp; [`r icons::icon_style(icons::fontawesome("link", style = "solid"), fill = "white")` University of Amsterdam](https://www.uva.nl/en/profile/z/w/i.m.zwetsloot/i.m.zwetsloot.html)<br/><br/>'
date: 'June 20, 2024 | JRC 2024 | Waterloo, Canada'
output:
  xaringan::moon_reader:
    self_contained: false
    css: [default, "style_files/fonts.css", "style_files/my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLanguage: ["r"]
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: true
header-includes:  
  - "style_files/header.html"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      progress = FALSE, 
                      verbose = FALSE,
                      dev = 'png',
                      fig.height = 3,
                      dpi = 300,
                      fig.align = 'center')

options(htmltools.dir.version = FALSE)


miamired = '#C3142D'

if(require(pacman)==FALSE) install.packages("pacman")
if(require(devtools)==FALSE) install.packages("devtools")
if(require(countdown)==FALSE) devtools::install_github("gadenbuie/countdown")
if(require(xaringanExtra)==FALSE) devtools::install_github("gadenbuie/xaringanExtra")
if(require(emo)==FALSE) devtools::install_github("hadley/emo")
if(require(icons)==FALSE) devtools::install_github("mitchelloharawild/icons")

pacman::p_load(tidyverse, magrittr, lubridate, janitor, # data analysis pkgs
               DataExplorer, scales, plotly, calendR, pdftools, # plots
               tmap, sf, urbnmapr, tigris, # maps
               bibliometrix, # for bibliometric analysis of my papers
               gifski, av, gganimate, ggtext, glue, extrafont, # for animations
               emojifont, emo, RefManageR, xaringanExtra, countdown) # for
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
if(require(xaringanthemer) == FALSE) install.packages("xaringanthemer")
library(xaringanthemer)

style_mono_accent(base_color = "#84d6d3",
                  base_font_size = "20px")

xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         
  mute_unhighlighted_code = TRUE  
)

xaringanExtra::use_xaringan_extra(c("tile_view", "animate_css", "tachyons", "panelset", "share_again", "search", "fit_screen", "editable", "clipboard"))

```


# The Road to Large Language Models

<br>

```{r generative_ai_chart_out, out.width='100%', dpi = 600, fig.alt='From big data to big models, a flow chart documenting how we got to large language models'}
knitr::include_graphics('figs/generative_ai_chart.png')
```

.footnote[
<html>
<hr>
</html>

**Comment:** You have been hearing about **big data** in SPC for over a decade now. In fact, we presented our paper titled [Statistical Perspectives on Big Data](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ab40f392e653b7336cbebf7c4fb95d3988748282) was presented almost exactly 11 years ago in the ISQC Workshop in Sydney. We now have models that can digest and generate answers based on more than 45TB of text. 
]

---

# Uniqueness of LLMs vs. Earlier AI Models

.content-box-gray[
.bold[.red[LLMs:]] .bold[The objective is to generate new content rather than analyze existing data.]
]

.font90[
-  The generated content is based on a .bold[.red[stochastic behavior embedded in generative AI models such that the same input prompts results in different content]].
- LLMs with known model sizes can have up to **540 billion parameters** ([PaLM](https://arxiv.org/abs/2204.02311)). Note that state-of-the-art models like *GPT-4o*, *PaLM 2* and *Claude Opus* **have not revealed their model sizes**.
- With the increase in model size, researchers have observed the **‚Äúemergent abilities‚Äù** of LLMs, which were **not explicitly encoded in the training**. [Examples include](https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html):
  + Multi-step arithmetic, and  
  + taking college-level exams.  
- LLMs are **foundation models** (see [Bommasani et al. 2021](https://arxiv.org/abs/2108.07258)), large pre-trained AI systems
that can be **repurposed with minimal effort across numerous domains and diverse tasks.**
]


---

# Generative AI Hype (2023)

```{r mckinsey_ai, echo=FALSE, out.width='60%'}
knitr::include_graphics('figs/mckinsey_ai.png')
```

.footnote[
<html>
<hr>
</html>

**Image Source:** [McKinsey & Company (July 2023). The economic potential of generative AI: The next productivity frontier [P. 10]](https://www.mckinsey.com/~/media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/the%20economic%20potential%20of%20generative%20ai%20the%20next%20productivity%20frontier/the-economic-potential-of-generative-ai-the-next-productivity-frontier-vf.pdf)
]


---

# Generative AI Hype (2024)

```{r google_ai, echo=FALSE, out.width='50%'}
knitr::include_graphics('figs/google_ai.png')
```

.footnote[
<html>
<hr>
</html>

**Image Source:** [Andrew McAfee (2024). Generally Faster: The Economic Impact of Generative AI [P. 11]](https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/Generally_Faster_-_The_Economic_Impact_of_Generative_AI.pdf)
]


---

#ü§¶üèΩ‚Äç‚ôÇÔ∏èÔ∏è But also My Experience in June of 2024

```{r rock_paper, echo=FALSE, out.width='63%'}
knitr::include_graphics('figs/rock_paper_scissors.png')
```

.footnote[
<html>
<hr>
</html>

**Source:** Megahed, F.M. (2024). [Playing Rock Paper Scissors with ChatGPT 4o on June 12, 2024.](https://chatgpt.com/share/45581b1d-6b16-4345-a546-9c8f4fd2d0f8)
]


---
class: inverse, center, middle

# On the Use of LLMs, such as ChatGPT, in SQC

<br>

.pull-left-2[<br>Megahed, F. M., Chen, Y. J., Ferris, J. A., Knoth, S., & Jones-Farmer, L. A. (2024). How generative AI models such as ChatGPT can be (mis)used in SPC practice, education, and research? An exploratory study. *Quality Engineering*, 36(2), 287‚Äì315.

[Freely available @ [arXiv](https://arxiv.org/pdf/2302.10916.pdf)].]

.pull-right-2[<div><img src="figs/paper_qr_code.png" class="paper-img" width="300px" align="right"></div>]


---

# Our Overarching Research Question

.content-box-red[
.bold[What can generative LLM-based AI tools do now to augment the roles of SPC practitioners, educators, and researchers?]

]

-  **Secondary goal:** To motivate the SPC community to be receptive to exploring whether new AI tools can help them be more **efficient**, **productive**, and **innovative**. This is consistent with:

  + Box and Woodall ([2012](https://www.tandfonline.com/doi/10.1080/08982112.2012.627003)): ‚Äúwe stress the necessity for the quality engineering community to strengthen and promote its role in **innovation**‚Äù, and  
  + Hockman and Jensen ([2016](https://www.tandfonline.com/doi/10.1080/08982112.2015.1083107)): ‚Äúfor statisticians to be successful in leading innovation, they will need to strengthen their **skills beyond what they have traditionally needed in the past**, but we believe this will be worth the effort‚Äù.  
  
- **Scope:** We evaluated the utility of ChatGPT (GPT-3.5 engine) as of its *Jan 30, 2023 Version*.   

---

# Our Study Design

```{r study_design, include=FALSE}
pdftools::pdf_convert('figs/methods.pdf', dpi = 600,
                      filenames = 'figs/methods.png') # convert latex pdf to png
```
<center>
<img src="./figs/methods.png" alt="An overview of our study design, where we focused on three applications code, explanation, and knowledge generation for each application domain of practice, learning, and research. Red color is used to highlight the questions that will be discussed in the presentation." width="70%" height="70%" border="0" style="padding:0px; display: block; line-height: 0px; font-size: 0px; border:0px;" />
</center>


---

# The Good: Knowledge Generation


.bold[Inspired by the TEDxBoston talk titled [what we learned from 5 million books](https://www.ted.com/talks/jean_baptiste_michel_erez_lieberman_aiden_what_we_learned_from_5_million_books?language=en), we asked ChatGPT the following question:]

<br>

> .bold[.large["What are open issues in statistical process control research?'']]

<br>

### Why this question seemed like a reasonable prompt?
.bold[ChatGPT likely ‚Äúread‚Äù and ‚Äúcan recall‚Äù more SPC research papers than most of us] 

---


# The Good: Knowledge Generation

```{r spc_research_knowledge, echo=FALSE, out.width='63%', fig.alt='Chat GPT highlighted six areas where there are open issues in statistical process control. We will highlight the main themes in the next slide'}
knitr::include_graphics('figs/research_prompt_08_fig_01.png')
```


---


# The Good: Knowledge Generation

.content-box-red[
.center[.bold[.large[Some Thoughts on the ChatGPT Answer]]]

- It captured .bold[reasonable themes, e.g., ]
  + incorporating .bold[big data and machine learning] techniques, 
  + .bold[online/real-time monitoring] solutions where 100% sampling is employed, 
  + the need for .bold[non-normality], and 
  + .bold[applications to new domains].  

- In our opinion, .bold[value is in using it as a high-level tool for idea generation/validation].  
- Potentially .bold[‚Äústale‚Äù] as [Chat(GPT)-3.5 ‚Äúfinished training in early 2022‚Äù](https://openai.com/blog/chatgpt/) and is limited to [data up to Sept 2021](https://community.openai.com/t/knowledge-cutoff-date-of-september-2021/66215).  
  + Probably not an issue for future LLM generations (.bold[Why?])
]




---

# The Bad: Precise Definitions

```{r imprecise, out.width='46%', echo=FALSE, fig.alt="ChatGPT's generated response for our prompt of explain the practitioner-to-practitioner variability. Its response is somewhat long and imprecise. Specifically, ChatGPT presented five factors, which share a common feature; all deal with differences on the method level, i.e., chart type, subgroup design, techniques to calculate the limits, dealing with outliers, and choice of software. While we agree that these factors are important and will drive different results, ChatGPT's answer ignores the context for which the practitioner-to-practitioner variability is used in the SPC literature. In fact, the practitioner-to-practitioner variability refers to the variation that occurs with a fixed configuration of the five aforementioned factors, i.e., the variation results from multiple implementations of the same procedure on the same data-generating process."}
knitr::include_graphics('figs/research_prompt_05_fig_01.png')
```



---

# The Ugly: ChatGPT's Hallucination

.bold[To detect whether ChatGPT can detect erroneous requests, we asked:]

<br>

> .bold[.large["Can you use the ‚Äòbigfish' dataset from the qcc library in R to create a control chart?'']]

<br>

### Why this question seemed like a reasonable prompt?
.bold[In an earlier question (within the same thread), ChatGPT answered a question by using the `qcc` package, i.e., is .red[familiar with it], and .red[detecting unreasonable requests would be a strong feature for non-expert users].]


---


# The Ugly: ChatGPT's Hallucination

```{r bigfish1, out.width='60%', echo=FALSE, fig.alt='The ChatGPT hallucination, answering a question about a non-existent dataset in the qcc library'}
knitr::include_graphics('figs/practice_prompt_03_fig_01.png')
```


---


# The Ugly: ChatGPT's Hallucination

```{r bigfish2, out.width='80%', echo=FALSE, fig.alt='ChatGPT making up details about the non-existent bigfish dataset and saying it is popular in the SPC community'}
knitr::include_graphics('figs/practice_prompt_03_fig_02.png')
```


---

# The Ugly: ChatGPT's Hallucination (GPT-4o)

<div style="margin-top: -10px;">
    <center>
        <video style="width: 100%; max-height: 63vh;" controls>
            <source src="figs/bigfish.mp4" type="video/mp4">
        </video>
    </center>
</div>


.footnote[
<html>
<hr>
</html>

**Note:** This trial was performed solely for our presentation. The model used herein should be a **much improved model** compared to the 3.5-model examined in the original paper. Yet, the **hallucination** has remained.
]

---
class: inverse, center, middle

# ChatSQC: Our Grounded App, to address Imprecise SQC Answers and Hallucinations

<br>

.pull-left-2[<br>Megahed, F. M., Chen, Y. J., Zwetsloot, I., Knoth, S., Montgomery, D.C., & Jones-Farmer, L. A. (2024). Introducing ChatSQC: Enhancing Statistical Quality Control with Augmented AI. Under second review.

[Freely available @ [arXiv](https://arxiv.org/pdf/2308.13550)].]

.pull-right-2[<div><img src="figs/paper2_qr_code.png" class="paper-img" width="300px" align="right"></div>]



---

# The Construction of ChatSQC

```{r chatsqc, out.width='80%', echo=FALSE, fig.alt='The construction of ChatSQC involved four main phases: (a) a one-time extraction of the reference material, (b) a one-time preprocessing of the extracted material, (c) a continuous (online) chat inference, and (d) the hosting/deployment of the app on a web server.'}
knitr::include_graphics('figs/ChatSQC_flowchart_new.png')
```

---

# A Live Demo of ChatSQC

<center>
    <a href="https://chatsqc.osc.edu/">
        <img alt="The interface to our ChatSQC app" src="figs/chatsqc_demo.png" style="width:100%; height:100%;">
    </a>
</center>

.footnote[
<html>
<hr>
</html>

**Note:** We encourage the audience to experiment with **ChatSQC** at <https://chatsqc.osc.edu/>.
]

---
class: inverse, center, middle

# How Can Industrial Statistics Inform LLM Usage and Evaluation? Some Initial Thoughts

<br>

<br>Megahed, F. M., Chen, Y. J., Jones-Farmer, L. A., Knoth, S., Lee, Y., Montgomery, D.C.,  &  Wang, B., Zwetsloot, I. (2024). Work In Progress.

---

# LLM Usage in Business and Industry

```{r use_cases, include = FALSE, out.width='70%'}
graph = DiagrammeR::grViz("
  digraph LLM_Use_Cases {
    graph [fontsize=12, overlap=true]

    node [shape = box, fontname = Arial, fontcolor = black]

    UseCases [label = <<B>Use Cases</B>>, fontsize=16]
    HighlyUseful [label = <<B>Highly Useful </B>>, fontsize=14]
    SomewhatUseful [label = <<B>Somewhat Useful </B>>, fontsize=14]
    HardlyUseful [label = <<B>Hardly Useful </B>>, fontsize=14]

    ContentCreation [label = 'Content\nCreation']
    Chatbots [label = 'Chatbots']
    KnowledgeDiscovery [label = 'Knowledge\nDiscovery']

    SegmentationClassification [label = 'Segmentation/\nClassification\nand\n Recommendation\nSystems', fontcolor = '#C3142D', fontname = Arial, style = bold, penwidth = 4, color = '#C3142D', fontface = 'bold', fillcolor='white', style ='filled']
    IntelligentAutomation [label = 'Intelligent\nAutomation']
    AnomalyDetection [label = 'Anomaly\nDetection\nand\nMonitoring']

    PredictionForecasting [label = 'Prediction/\nForecasting']
    Planning [label = 'Planning']
    DecisionIntelligence [label = 'Decision\nIntelligence']

    UseCases -> HighlyUseful
    UseCases -> SomewhatUseful
    UseCases -> HardlyUseful

    HighlyUseful -> ContentCreation
    HighlyUseful -> Chatbots
    HighlyUseful -> KnowledgeDiscovery

    SomewhatUseful -> SegmentationClassification
    SomewhatUseful -> IntelligentAutomation
    SomewhatUseful -> AnomalyDetection

    HardlyUseful -> PredictionForecasting
    HardlyUseful -> Planning
    HardlyUseful -> DecisionIntelligence
  }
")

graph |> DiagrammeRsvg::export_svg() |> xml2::read_xml() |> 
  xml2::write_xml("figs/use_cases.svg")
```


```{r use_cases_out, echo=FALSE, out.width='100%'}
knitr::include_graphics("figs/use_cases.svg") 
```

.footnote[
<html>
<hr>
</html>

**Created By:** Fadel Megahed based on the text in the article by Ava McCartney. (2024). "When Not to Use Generative AI", *Gartner*. The article was published on April 23, 2024 and last accessed on June 14, 2024. It can be accessed at <https://www.gartner.com/en/articles/when-not-to-use-generative-ai>.
]

---

# Current Research on LLM-based Classification

```{r three_papers_crop, echo=FALSE, out.width='70%'}
knitr::include_graphics("figs/adjusted_paper_info.gif")
```

.footnote[
<html>
<hr>
</html>

**Note:** The paper by [Eisfeldt et al. (2023)](https://www.nber.org/papers/w31222) is what led to our collaboration with Brooke Wang and started our latest work in this area.
]

---

# On "Generative AI and Firm Values"

```{r andrea_paper, include=FALSE}
graph2 = DiagrammeR::grViz("
digraph flowchart {
  node [fontsize=12,shape = box, fontname = Arial]
  
  InputJobsTasks [label = 'Job-Task Pairings']
  InputSystemPrompt [label = 'System Prompt along with a task classification rubric']
  ToolChatGPT [label = <<b>ChatGPT 3.5</b>>, penwidth = 2, fillcolor = 'white', color = '#C3142D', styled = 'filled']
  OutputClassification [label = <<b>AI exposure classification for each task     </b>>, penwidth = 4]
  
  InputJobsTasks -> ToolChatGPT [label = <<b>Input</b>>, fontcolor = '#C3142D']
  InputSystemPrompt -> ToolChatGPT [label = <<b>Input</b>>, fontcolor = '#C3142D']
  ToolChatGPT -> OutputClassification [label = <<b>Generation</b>>, fontcolor = '#C3142D']
  
  OutputClassification -> E0 [arrowhead = none, style = 'dashed, dot', color = 'gray']
  OutputClassification -> E1 [arrowhead = none, style = 'dashed, dot', color = 'gray']
  OutputClassification -> E2 [arrowhead = none, style = 'dashed, dot', color = 'gray']
  OutputClassification -> E3 [arrowhead = none, style = 'dashed, dot', color = 'gray']
  
  E0 [label = 'E0: No exposure', penwidth = 1, color = 'gray', fillcolor = white, style = 'filled, dashed']
  E1 [label = 'E1: Direct Exposure', penwidth = 1, color = 'gray', fillcolor = white, style = 'filled, dashed']
  E2 [label = 'E2: Exposure by LLM-powered applications', penwidth = 1, color = 'gray', fillcolor = white, style = 'filled, dashed']
  E3 [label = 'E3: Exposure given image capabilities', penwidth = 1, color = 'gray', fillcolor = white, style = 'filled, dashed']
}
")

graph2 |> DiagrammeRsvg::export_svg() |> xml2::read_xml() |> 
  xml2::write_xml("figs/andrea_paper.svg")
```

```{r andrea_paper_output, echo=FALSE, out.width='100%'}
knitr::include_graphics("figs/andrea_paper.svg")
```

.footnote[
<html>
<hr>
</html>

**Note:** Our best attempt to summarize the work of Eisfeldt et al. (2023). "Generative AI and Firm Values". Available from <https://www.nber.org/papers/w31222>. Note that this chart was created by Fadel Megahed and that the authors randomly passed job-task pairings into ChatGPT and repeated this process 3 times to measure its consistency for classification.
]


---

# Our Future Work in this Area

1. Evaluating the **reliability of LLMs in automated text labeling:**  
  - How can we rigorously evaluate the reliability of large language models (LLMs) both within a single model (intra-model) and across different models (inter-model)?  
  - Consider the unequal costs associated with different model runs.  
  - Predetermine the number of runs, replicates, and other design of experiments (DoE) factors before starting the experiments.  
  - Consider different text labeling scenarios.
  
  
2. Optimizing **LLM Evaluations:**  
  - What comprehensive evaluation strategy should practitioners and applied researchers use to optimize their evaluations of LLMs?  
  - Incorporate factors such as cost variations, performance rankings, and consistency results. 


---

class: inverse, center, middle

# Three Final Thoughts


---

# 1. Keeping up with AI Developments is Hard!!

```{python generative_ai_dev, include=FALSE, eval = FALSE}
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.animation import FuncAnimation
from datetime import datetime

# Data for the timeline
events = [
    ("GPT-2", "February 2019", "One of the first large language models, 1.5 billion parameters. "
                               "Impressive text generation capabilities."),
    ("GPT-3", "June 2020", "175 billion parameters, achieved state-of-the-art results on many "
                           "NLP benchmarks. Enabled coherent, contextual text generation."),
    ("DALL-E", "January 2021", "OpenAI's first multimodal AI for generating images from text "
                               "descriptions. Pioneered diffusion models for image synthesis."),
    ("CLIP", "June 2021", "Contrastive language-image pre-training model that enabled zero-shot "
                          "transfer for vision tasks. A breakthrough in multimodal learning."),
    ("DALL-E 2", "April 2022", "Improved image generation with higher resolution, better coherence, "
                               "and ability to create variations."),
    ("ChatGPT", "November 2022", "Conversational AI assistant based on GPT-3.5, with improved dialogue "
                                 "abilities and instruction following. Gained widespread popularity."),
    ("Whisper", "September 2022", "Automatic speech recognition (ASR) system that outperformed previous "
                                  "models while being more robust and efficient."),
    ("GPT-4", "March 2023", "Multimodal model with rumoured 1.76 trillion parameters, capable of analyzing images "
                            "and documents in addition to text. Significant performance gains."),
    ("DALL-E 3", "November 2023", "Next-generation multimodal model with advanced image editing, animation, "
                                  "and 3D capabilities. Integrated with ChatGPT."),
    ("GPT-4 Turbo", "November 2023", "Optimized GPT-4 with faster performance, lower costs, larger context "
                                     "window, trained on newer data."),
    ("GPT-4o", "May 2024", "New AI model released, with improved text, video, and audio processing "
                           "capabilities over GPT-4. Integrated into ChatGPT desktop app.")
]

# Sort events by date
events.sort(key=lambda x: datetime.strptime(x[1], "%B %Y"))

# Categorize by model types
categories = {
    "LLM": ["GPT-2", "GPT-3", "GPT-4", "GPT-4 Turbo", "GPT-4o"],
    "Image": ["DALL-E", "DALL-E 2", "DALL-E 3", "CLIP"],
    "Voice": ["Whisper"],
    "ChatGPT": ["ChatGPT"]
}

# Extracting dates and descriptions
dates = [datetime.strptime(event[1], "%B %Y") for event in events]
descriptions = [f"{event[0]}: {event[2]}" for event in events]
y_positions = [0]*len(events)
colors = []
sizes = []

for i, event in enumerate(events):
    if event[0] in categories["LLM"]:
        y_positions[i] = 1
        colors.append("deepskyblue")
        sizes.append(200)
    elif event[0] in categories["Image"]:
        y_positions[i] = 2
        colors.append("mediumseagreen")
        sizes.append(200)
    elif event[0] in categories["Voice"]:
        y_positions[i] = 3
        colors.append("tomato")
        sizes.append(200)
    else:
        y_positions[i] = 4
        colors.append("gold")
        sizes.append(400)

# Create figure and axis
fig, ax = plt.subplots(figsize=(12, 8))

# Function to update the plot
def update(num):
    ax.clear()
    ax.scatter(dates[:num+1], y_positions[:num+1], color=colors[:num+1], s=sizes[:num+1])
    ax.set_yticks([1, 2, 3, 4])
    ax.set_yticklabels(["LLM", "Image", "Voice", "ChatGPT"], fontname='Comic Sans MS', fontsize=10, weight='bold')
    
    # Displaying the current event description
    if num < len(descriptions):
        desc = descriptions[num]
        words = desc.split()
        lines = []
        line = ""
        for word in words:
            if len(line) + len(word) + 1 <= 80:
                line += (word + " ")
            else:
                lines.append(line)
                line = word + " "
        lines.append(line)
        wrapped_desc = "\n".join(lines)
        
        # Adjusting text position to ensure it's within plot boundaries
        x_pos = dates[num]
        y_pos = y_positions[num] + 0.25
        text_x_pos = x_pos
        if num in [0]:
            text_x_pos = dates[num] + (dates[1] - dates[0]) * 0.3  # Shift right for the first two descriptions
        elif num in [len(descriptions) - 2, len(descriptions) - 1]:
            text_x_pos = dates[num] - (dates[num] - dates[num-1]) * 1.4  # Shift left for the last two descriptions
        
        ax.text(text_x_pos, y_pos, wrapped_desc, ha='center', va='center', fontsize=10, fontname='DejaVu Sans', weight='bold')
        # Draw arrow from description to point
        ax.annotate('', xy=(x_pos, y_positions[num]), xytext=(text_x_pos, y_pos - 0.1 * (len(lines) - 1)),
            arrowprops=dict(arrowstyle="->", color='black'))

    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))
    plt.setp(ax.get_xticklabels(), ha="right", fontname='Comic Sans MS', fontsize=10, weight='bold')
    ax.set_xlim([datetime(2018, 7, 1), datetime(2024, 12, 31)])
    ax.set_ylim(0.5, 4.5)
    ax.set_title('A Timeline of Major AI Model Releases by OpenAI', fontname='DejaVu Sans', fontsize=12, weight='bold')
    ax.text(0.5, .985, 'Rapid advancements in AI technologies by OpenAI, highlighting key model releases.', 
        transform=ax.transAxes, ha='center', va='center', fontsize=10, color='darkgray', weight='bold', fontname='DejaVu Sans')
    ax.annotate("Created By: Fadel Megahed | Data Source: Publicly available data, which was compiled by the author", xy=(0.05, 0.01), xycoords='figure fraction', ha='left', fontsize=10, fontname='DejaVu Sans')
    ax.yaxis.grid(False)
    ax.xaxis.grid(False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)

# Create animation
ani = FuncAnimation(fig, update, frames=len(dates)+1, repeat=False)

# Save or show the animation
ani.save('timeline_animation.mp4', writer='ffmpeg', fps=0.333)
# plt.show()  # Uncomment this line to display the animation instead of saving it
```

<div style="margin-top: -35px;">
    <center>
        <video style="width: 100%; max-height: 50vh;" controls>
            <source src="figs/timeline_animation.mp4" type="video/mp4">
        </video>
    </center>
</div>


---

# 2. Use Cases Overlap with our Discpline!!

```{r use_cases2, include = FALSE, out.width='70%'}
graph = DiagrammeR::grViz("
  digraph LLM_Use_Cases {
    graph [fontsize=12, overlap=true]

    node [shape = box, fontname = Arial, fontcolor = black]

    UseCases [label = <<B>Use Cases</B>>, fontsize=16]
    HighlyUseful [label = <<B>Highly Useful </B>>, fontsize=14]
    SomewhatUseful [label = <<B>Somewhat Useful </B>>, fontsize=14]
    HardlyUseful [label = <<B>Hardly Useful </B>>, fontsize=14]

    ContentCreation [label = 'Content\nCreation']
    Chatbots [label = 'Chatbots']
    KnowledgeDiscovery [label = 'Knowledge\nDiscovery']

    SegmentationClassification [label = 'Segmentation/\nClassification\nand\n Recommendation\nSystems']
    IntelligentAutomation [label = 'Intelligent\nAutomation']
    AnomalyDetection [label = 'Anomaly\nDetection\nand\nMonitoring', fontcolor = '#C3142D', fontname = Arial, style = bold, penwidth = 4, color = '#C3142D', fontface = 'bold', fillcolor='white', style ='filled']

    PredictionForecasting [label = 'Prediction/\nForecasting']
    Planning [label = 'Planning']
    DecisionIntelligence [label = 'Decision\nIntelligence']

    UseCases -> HighlyUseful
    UseCases -> SomewhatUseful
    UseCases -> HardlyUseful

    HighlyUseful -> ContentCreation
    HighlyUseful -> Chatbots
    HighlyUseful -> KnowledgeDiscovery

    SomewhatUseful -> SegmentationClassification
    SomewhatUseful -> IntelligentAutomation
    SomewhatUseful -> AnomalyDetection

    HardlyUseful -> PredictionForecasting
    HardlyUseful -> Planning
    HardlyUseful -> DecisionIntelligence
  }
")

graph |> DiagrammeRsvg::export_svg() |> xml2::read_xml() |> 
  xml2::write_xml("figs/use_cases2.svg")
```


```{r use_cases2_out, echo=FALSE, out.width='100%'}
knitr::include_graphics("figs/use_cases2.svg") 
```

.footnote[
<html>
<hr>
</html>

**Created By:** Fadel Megahed based on the text in the article by Ava McCartney. (2024). "When Not to Use Generative AI", *Gartner*. The article was published on April 23, 2024 and last accessed on June 14, 2024. It can be accessed at <https://www.gartner.com/en/articles/when-not-to-use-generative-ai>.
]


---

# 3. AI and Statistics: Perfect Together!!

```{r ai_stats_perfect, echo=FALSE, out.width='60%'}
knitr::include_graphics("figs/ai_stats_perfect.png") 
```

.footnote[
<html>
<hr>
</html>

**Source:** Redman, T.C., and Hoerl, R.W. (2024). "AI and Statistics: Perfect Together". *MIT Sloan Management Review*, available at <https://sloanreview.mit.edu/article/ai-and-statistics-perfect-together/>.
]


---
class: center, middle, inverse, title-slide

.title[
# <p>Industrial Satistics and Large Language Models</p>
]
.author[
### Fadel M. Megahed, PhD <br><a href="https://scholar.google.com/citations?user=6CTlKGMAAAAJ&amp;hl=en&amp;oi=ao"><svg viewBox="0 0 488 512" style="height:1em;position:relative;display:inline-block;top:.1em;fill:white;" xmlns="http://www.w3.org/2000/svg"> <path d="M488 261.8C488 403.3 391.1 504 248 504 110.8 504 0 393.2 0 256S110.8 8 248 8c66.8 0 123 24.5 166.3 64.9l-67.5 64.9C258.5 52.6 94.3 116.6 94.3 256c0 86.5 69.1 156.6 153.7 156.6 98.2 0 135-70.4 140.8-106.9H248v-85.3h236.1c2.3 12.7 3.9 24.9 3.9 41.4z"></path></svg> Scholar</a>   |   <a href="https://twitter.com/FadelMegahed"><svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;fill:white;" xmlns="http://www.w3.org/2000/svg"> <path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg> <span class="citation">@FadelMegahed</span></a>   |   <a href="https://github.com/fmegahed"><svg viewBox="0 0 496 512" style="height:1em;position:relative;display:inline-block;top:.1em;fill:white;" xmlns="http://www.w3.org/2000/svg"> <path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg> <span class="citation">@fmegahed</span></a>   |   <a href="mailto:fmegahed@miamioh.edu"><svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;fill:white;" xmlns="http://www.w3.org/2000/svg"> <path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"></path></svg> fmegahed@miamioh.edu</a></br> <br> <u><b><font color="white">Joint work with:</b></u><br> Ying-Ju (Tessa) Chen, PhD   <a href="https://udayton.edu/directory/artssciences/mathematics/chen-ying-ju.php"><svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;fill:white;" xmlns="http://www.w3.org/2000/svg"> <path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg> University of Dayton</a><br/> Allison Jones-Farmer, PhD   <a href="https://miamioh.edu/fsb/directory/?up=/directory/farmerl2"><svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;fill:white;" xmlns="http://www.w3.org/2000/svg"> <path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg> Miami University</a><br> Sven Knoth, PhD   <a href="https://www.hsu-hh.de/compstat/en/sven-knoth-2"><svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;fill:white;" xmlns="http://www.w3.org/2000/svg"> <path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg> Helmut-Schmidt-Universit√§t</a><br/> Younghwa (Gabe) Lee, PhD   <a href="https://miamioh.edu/fsb/directory/?up=/directory/leeyh2"><svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;fill:white;" xmlns="http://www.w3.org/2000/svg"> <path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg> Miami University</a><br> Douglas C. Montgomery, PhD   <a href="https://search.asu.edu/profile/10123"><svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;fill:white;" xmlns="http://www.w3.org/2000/svg"> <path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg> Arizona State University</a><br> Brooke Wang, PhD   <a href="https://miamioh.edu/fsb/directory/?up=/directory/wangj249"><svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;fill:white;" xmlns="http://www.w3.org/2000/svg"> <path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg> Miami University</a><br> Inez Zwetsloot, PhD   <a href="https://www.uva.nl/en/profile/z/w/i.m.zwetsloot/i.m.zwetsloot.html"><svg viewBox="0 0 512 512" style="height:1em;position:relative;display:inline-block;top:.1em;fill:white;" xmlns="http://www.w3.org/2000/svg"> <path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg> University of Amsterdam</a><br/><br/>
]
.date[
### June 20, 2024 | JRC 2024 | Waterloo, Canada
]